{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Importaciones y Dependencias\n","\n"],"metadata":{"id":"edWfjq5nfTSA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Ifr_trhlvEV"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from scipy.special import erf  # Para calcular la función de error"]},{"cell_type":"markdown","source":["# Funciones de Activación"],"metadata":{"id":"CrdAs29JlyGc"}},{"cell_type":"code","source":["def sigmoid(z):\n","    z = np.clip(z, -500, 500)\n","    den = (1 + np.exp(-z))\n","    den[den == 0] = 1\n","    return 1 / den\n","\n","def sigmoid_derivative(z):\n","    return sigmoid(z) * (1 - sigmoid(z))\n","\n","def softmax(z):\n","    exp_z = np.exp(z - np.max(z))  # Evita overflow\n","    if z.ndim == 1:  # Si z es un vector unidimensional\n","        sum_exp_z = np.sum(exp_z)\n","        sum_exp_z[sum_exp_z == 0] = 1\n","        return exp_z / sum_exp_z\n","    else:  # Si z es una matriz bidimensional\n","        sum_exp_z = np.sum(exp_z, axis=1, keepdims=True)\n","        # Asegurar que el denominador no sea cero\n","        sum_exp_z[sum_exp_z == 0] = 1  # Esto evita la división por cero\n","        return exp_z / sum_exp_z\n","\n","def tanh(z):\n","    return np.tanh(z)\n","\n","def tanh_derivative(z):\n","    return 1 - np.tanh(z) ** 2\n","\n","def relu(z):\n","    return np.maximum(0, z)\n","\n","def relu_derivative(z):\n","    return np.where(z > 0, 1, 0)\n","\n","def leaky_relu(z, alpha=0.01):\n","    return np.where(z > 0, z, alpha * z)\n","\n","def leaky_relu_derivative(z, alpha=0.01):\n","    return np.where(z > 0, 1, alpha)\n","\n","def elu(z, alpha=1.0):\n","    return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n","\n","def elu_derivative(z, alpha=1.0):\n","    return np.where(z > 0, 1, alpha * np.exp(z))\n","\n","def swish(z):\n","    return z * sigmoid(z)\n","\n","def swish_derivative(z):\n","    sig = sigmoid(z)\n","    return sig + z * sig * (1 - sig)\n","\n","def gelu(z):\n","    return 0.5 * z * (1 + erf(z / np.sqrt(2)))\n","\n","def gelu_derivative(z):\n","    return 0.5 * (1 + erf(z / np.sqrt(2))) + (z / np.sqrt(2 * np.pi)) * np.exp(-0.5 * z**2)\n"],"metadata":{"id":"_Aa4NA0elx5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Funciones de Optimización"],"metadata":{"id":"wklg2IFCXYt1"}},{"cell_type":"code","source":["def gradient_descent_update(weights, grads, learning_rate):\n","    return weights - learning_rate * grads\n","\n","def adam_update(weights, grads, v, s, t, beta1, beta2, learning_rate, epsilon):\n","    # Actualiza los momentos de primer y segundo orden\n","    v = beta1 * v + (1 - beta1) * grads\n","    s = beta2 * s + (1 - beta2) * (grads ** 2)\n","\n","    # Corrección de sesgo\n","    v_hat = v / (1 - beta1 ** t)\n","    s_hat = s / (1 - beta2 ** t)\n","\n","    # Actualización de pesos\n","    weights -= learning_rate * v_hat / (np.sqrt(s_hat) + epsilon)\n","    return weights, v, s\n","\n","def rmsprop_update(weights, grads, cache, rho, learning_rate, epsilon):\n","    # Actualiza el cache de los gradientes\n","    cache = rho * cache + (1 - rho) * (grads ** 2)\n","\n","    # Actualización de pesos\n","    weights -= learning_rate * grads / (np.sqrt(cache) + epsilon)\n","    return weights, cache"],"metadata":{"id":"R9-ShCVrXbNJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Carga y Preprocesamiento de Datos"],"metadata":{"id":"HUA5pRc0_tn0"}},{"cell_type":"code","source":["def normalizar_caracteristicas(X):\n","    media = X.mean(axis=0)\n","    desviacion = X.std(axis=0)\n","    desviacion[desviacion == 0] = 1\n","    return (X - media) / desviacion\n","\n","def one_hot(y, num_classes):\n","    return np.eye(num_classes)[y]\n","\n","def load_csv(file_path):\n","    df = pd.read_csv(file_path)\n","    label_to_int = {'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}\n","    df['variety'] = df['variety'].map(label_to_int)\n","    X = df.iloc[:, :-1].values\n","    y = df.iloc[:, -1].values\n","    return X, y\n","\n","def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15):\n","    assert train_size + val_size + test_size == 1, \"La suma de los tamaños debe ser 1.\"\n","    indices = np.random.permutation(X.shape[0])\n","    train_set_size = int(X.shape[0] * train_size)\n","    val_set_size = int(X.shape[0] * val_size)\n","    train_indices = indices[:train_set_size]\n","    val_indices = indices[train_set_size:train_set_size + val_set_size]\n","    test_indices = indices[train_set_size + val_set_size:]\n","    return X[train_indices], X[val_indices], X[test_indices], y[train_indices], y[val_indices], y[test_indices]\n"],"metadata":{"id":"p2HyhlMW_q9r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Definición de la Clase Optimizer"],"metadata":{"id":"e2WXF4yy51jJ"}},{"cell_type":"code","source":["class Optimizer:\n","    def __init__(self, optimizer_type, learning_rate=0.01, beta1=0.9, beta2=0.999, rho=0.9, epsilon=1e-8):\n","        self.optimizer_type = optimizer_type\n","        self.learning_rate = learning_rate\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.rho = rho\n","        self.epsilon = epsilon\n","        self.t = 0\n","        self.v = {}\n","        self.s = {}\n","        self.cache = {}\n","\n","    def update(self, weights, grads):\n","        if self.optimizer_type == 'gd':  # Gradient Descent\n","            return self.gradient_descent(weights, grads)\n","        elif self.optimizer_type == 'adam':  # Adam optimizer\n","            return self.adam(weights, grads)\n","        elif self.optimizer_type == 'rmsprop':  # RMSprop\n","            return self.rmsprop(weights, grads)\n","\n","    def gradient_descent(self, weights, grads):\n","        weights['W1'] -= self.learning_rate * grads['dW1']\n","        weights['b1'] -= self.learning_rate * grads['db1']\n","        weights['W2'] -= self.learning_rate * grads['dW2']\n","        weights['b2'] -= self.learning_rate * grads['db2']\n","        return weights\n","\n","    def adam(self, weights, grads):\n","        self.t += 1\n","        weights['W1'], self.v['W1'], self.s['W1'] = adam_update(weights['W1'], grads['dW1'], self.v.get('W1', np.zeros_like(grads['dW1'])), self.s.get('W1', np.zeros_like(grads['dW1'])), self.t, self.beta1, self.beta2, self.learning_rate, self.epsilon)\n","        weights['W2'], self.v['W2'], self.s['W2'] = adam_update(weights['W2'], grads['dW2'], self.v.get('W2', np.zeros_like(grads['dW2'])), self.s.get('W2', np.zeros_like(grads['dW2'])), self.t, self.beta1, self.beta2, self.learning_rate, self.epsilon)\n","        weights['b1'], self.v['b1'], self.s['b1'] = adam_update(weights['b1'], grads['db1'], self.v.get('b1', np.zeros_like(grads['db1'])), self.s.get('b1', np.zeros_like(grads['db1'])), self.t, self.beta1, self.beta2, self.learning_rate, self.epsilon)\n","        weights['b2'], self.v['b2'], self.s['b2'] = adam_update(weights['b2'], grads['db2'], self.v.get('b2', np.zeros_like(grads['db2'])), self.s.get('b2', np.zeros_like(grads['db2'])), self.t, self.beta1, self.beta2, self.learning_rate, self.epsilon)\n","        return weights\n","\n","    def rmsprop(self, weights, grads):\n","        weights['W1'], self.cache['W1'] = rmsprop_update(weights['W1'], grads['dW1'], self.cache.get('W1', np.zeros_like(grads['dW1'])), self.rho, self.learning_rate, self.epsilon)\n","        weights['W2'], self.cache['W2'] = rmsprop_update(weights['W2'], grads['dW2'], self.cache.get('W2', np.zeros_like(grads['dW2'])), self.rho, self.learning_rate, self.epsilon)\n","        weights['b1'], self.cache['b1'] = rmsprop_update(weights['b1'], grads['db1'], self.cache.get('b1', np.zeros_like(grads['db1'])), self.rho, self.learning_rate, self.epsilon)\n","        weights['b2'], self.cache['b2'] = rmsprop_update(weights['b2'], grads['db2'], self.cache.get('b2', np.zeros_like(grads['db2'])), self.rho, self.learning_rate, self.epsilon)\n","        return weights\n"],"metadata":{"id":"G8UufM4-5ymW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Definición de la Clase NeuralNetwork"],"metadata":{"id":"7dIgLH0RXt96"}},{"cell_type":"code","source":["class NeuralNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, activation, activation_derivative, optimizer_type):\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.activation = activation\n","        self.activation_derivative = activation_derivative\n","        self.optimizer = Optimizer(optimizer_type)  # Se crea el optimizador\n","        self.weights = self._initialize_weights()\n","\n","        # Inicialización de v y s para optimizadores que lo necesiten\n","        self.v = {'W1': np.zeros((self.input_size, self.hidden_size)),\n","                  'b1': np.zeros((1, self.hidden_size)),\n","                  'W2': np.zeros((self.hidden_size, self.output_size)),\n","                  'b2': np.zeros((1, self.output_size))}\n","        self.s = {'W1': np.zeros((self.input_size, self.hidden_size)),\n","                  'b1': np.zeros((1, self.hidden_size)),\n","                  'W2': np.zeros((self.hidden_size, self.output_size)),\n","                  'b2': np.zeros((1, self.output_size))}\n","        self.cache = {'W1': np.zeros((self.input_size, self.hidden_size)),\n","                      'b1': np.zeros((1, self.hidden_size)),\n","                      'W2': np.zeros((self.hidden_size, self.output_size)),\n","                      'b2': np.zeros((1, self.output_size))}\n","        self.t = 0  # Contador de pasos de tiempo para Adam\n","\n","    def _initialize_weights(self):\n","        W1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n","        b1 = np.zeros((1, self.hidden_size))\n","        W2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n","        b2 = np.zeros((1, self.output_size))\n","        return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n","\n","    def forward_propagation(self, X):\n","        W1, b1, W2, b2 = self.weights.values()\n","        Z1 = np.dot(X, W1) + b1\n","        A1 = self.activation(Z1)\n","        Z2 = np.dot(A1, W2) + b2\n","        A2 = softmax(Z2)\n","        return A2, (Z1, A1, Z2, A2)\n","\n","    def backward_propagation(self, X, y, cache):\n","        W1, b1, W2, b2 = self.weights.values()\n","        Z1, A1, Z2, A2 = cache\n","        m = X.shape[0]\n","\n","        dZ2 = A2 - y\n","        dW2 = np.dot(A1.T, dZ2) / m\n","        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n","\n","        dA1 = np.dot(dZ2, W2.T)\n","        dZ1 = dA1 * self.activation_derivative(A1)\n","        dW1 = np.dot(X.T, dZ1) / m\n","        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n","\n","        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n","\n","    def update_weights(self, grads, learning_rate):\n","        \"\"\" Actualiza los pesos utilizando el optimizador especificado. \"\"\"\n","        self.weights = self.optimizer.update(self.weights, grads)\n","\n","    def predict(self, X):\n","        y_pred, _ = self.forward_propagation(X)\n","        return np.argmax(y_pred, axis=1)\n","\n","    def train(self, X, y, X_val, y_val, epochs, learning_rate, patience):\n","        y_one_hot = one_hot(y, self.output_size)\n","        y_val_one_hot = one_hot(y_val, self.output_size)\n","\n","        loss_history = []\n","        accuracy_history = []\n","        val_loss_history = []\n","        val_accuracy_history = []\n","\n","        best_val_loss = float('inf')  # Pérdida mínima encontrada\n","        epochs_without_improvement = 0\n","\n","        for epoch in range(epochs):\n","            # Forward propagation\n","            y_pred, cache = self.forward_propagation(X)\n","\n","            # Calcula la pérdida y precisión en entrenamiento\n","            loss = -np.sum(y_one_hot * np.log(y_pred + 1e-8)) / X.shape[0]\n","            loss_history.append(loss)\n","\n","            y_pred_labels = np.argmax(y_pred, axis=1)\n","            accuracy = calculate_accuracy(y, y_pred_labels)\n","            accuracy_history.append(accuracy)\n","\n","            # Forward propagation en validación\n","            y_val_pred, _ = self.forward_propagation(X_val)\n","            val_loss = -np.sum(y_val_one_hot * np.log(y_val_pred + 1e-8)) / X_val.shape[0]\n","            val_loss_history.append(val_loss)\n","\n","            y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n","            val_accuracy = calculate_accuracy(y_val, y_val_pred_labels)\n","            val_accuracy_history.append(val_accuracy)\n","\n","            # Llamar a early stopping\n","            stop, best_val_loss, epochs_without_improvement = early_stopping(\n","                val_loss, best_val_loss, patience, epochs_without_improvement\n","            )\n","            if stop:\n","                print(f\"Parada temprana en la época {epoch + 1}. Mejor pérdida de validación: {best_val_loss:.4f}\")\n","                break\n","\n","            # Backward propagation\n","            grads = self.backward_propagation(X, y_one_hot, cache)\n","\n","            # Actualizar pesos utilizando el optimizador\n","            self.update_weights(grads, learning_rate)\n","\n","            # Mostrar progreso cada 100 épocas\n","            if epoch % 100 == 0:\n","                print(f\"Época {epoch}, Pérdida: {loss:.4f}, Precisión: {accuracy:.2f}%, \"\n","                      f\"Pérdida Validación: {val_loss:.4f}, Precisión Validación: {val_accuracy:.2f}%\")\n","\n","        return loss_history, accuracy_history, val_loss_history, val_accuracy_history\n"],"metadata":{"id":"uaJM_DmwXwex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Función de Precisión y Early Stopping"],"metadata":{"id":"h8lqkW4YyD-E"}},{"cell_type":"code","source":["def calculate_accuracy(y_true, y_pred):\n","    correct = np.sum(y_true == y_pred)\n","    total = y_true.shape[0]\n","    accuracy = correct / total\n","    return accuracy * 100\n","\n","def early_stopping(val_loss, best_val_loss, patience, epochs_without_improvement):\n","    if val_loss < best_val_loss:\n","        return False, val_loss, 0  # Mejora encontrada, reinicia el contador\n","    else:\n","        epochs_without_improvement += 1\n","        if epochs_without_improvement >= patience:\n","            return True, best_val_loss, epochs_without_improvement  # Detener\n","        return False, best_val_loss, epochs_without_improvement"],"metadata":{"id":"lQNMpSIdyC3E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Entrenamiento y Pruebas"],"metadata":{"id":"k0VI8Rv8X2WV"}},{"cell_type":"code","source":["# Definición de funciones de activación\n","activation_functions = {\n","    'sigmoid': (sigmoid, sigmoid_derivative),\n","    'tanh': (tanh, tanh_derivative),\n","    'relu': (relu, relu_derivative),\n","    'leaky_relu': (leaky_relu, leaky_relu_derivative),\n","    'elu': (elu, elu_derivative),\n","    'swish': (swish, swish_derivative),\n","    'gelu': (gelu, gelu_derivative)\n","}\n","\n","# Definir optimizadores, tasas de aprendizaje y tamaños de la capa oculta\n","optimizers = ['gd', 'adam', 'rmsprop']\n","learning_rates = [0.001, 0.01, 0.1]\n","hidden_sizes = [50, 100]\n","\n","# Cargar y normalizar los datos\n","X, y = load_csv(\"iris.csv\")\n","X = normalizar_caracteristicas(X)\n","X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y, train_size=0.8, val_size=0.1, test_size=0.1)\n","\n","# Recorrer todas las combinaciones de activación, optimizador, tasa de aprendizaje y tamaño de la capa oculta\n","for i, (activation_name, (activation, activation_derivative)) in enumerate(activation_functions.items()):\n","    for j, optimizer in enumerate(optimizers):\n","        for k, learning_rate in enumerate(learning_rates):\n","            for l, hidden_size in enumerate(hidden_sizes):\n","                print(f\"\\nEntrenando con optimizador: {optimizer}, función de activación: {activation_name}, \"\n","                      f\"tasa de aprendizaje: {learning_rate}, tamaño de capa oculta: {hidden_size}\")\n","\n","                # Inicializar la red neuronal\n","                nn = NeuralNetwork(input_size=4, hidden_size=hidden_size, output_size=3, activation=activation, activation_derivative=activation_derivative, optimizer_type=optimizer)\n","\n","                # Entrenar la red neuronal\n","                loss_history, accuracy_history, val_loss_history, val_accuracy_history = nn.train(\n","                    X_train, y_train, X_val, y_val, epochs=10000, learning_rate=learning_rate, patience=1000\n","                )\n","\n","                # Crear una figura con dos subgráficos: uno para la pérdida y otro para la precisión\n","                fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n","\n","                # Graficar la pérdida\n","                ax[0].plot(loss_history, label='Pérdida', color='red')\n","                ax[0].set_title(f\"Pérdida durante el Entrenamiento\\n({optimizer}, {activation_name})\\n\"\n","                                f\"lr={learning_rate}, hidden_size={hidden_size}\")\n","                ax[0].set_xlabel(\"Épocas\")\n","                ax[0].set_ylabel(\"Pérdida\")\n","                ax[0].legend()\n","\n","                # Graficar la precisión\n","                ax[1].plot(accuracy_history, label='Precisión', color='blue')\n","                ax[1].set_title(f\"Precisión durante el Entrenamiento\\n({optimizer}, {activation_name})\\n\"\n","                                f\"lr={learning_rate}, hidden_size={hidden_size}\")\n","                ax[1].set_xlabel(\"Épocas\")\n","                ax[1].set_ylabel(\"Precisión (%)\")\n","                ax[1].legend()\n","\n","                # Ajustar el espacio entre subgráficos\n","                plt.tight_layout()\n","\n","                # Mostrar la figura con ambos gráficos\n","                plt.show()\n","\n","                # Precisión en el conjunto de entrenamiento\n","                y_pred_train = nn.predict(X_train)\n","                train_accuracy = calculate_accuracy(y_train, y_pred_train)\n","                print(f\"Precisión en el conjunto de entrenamiento con {optimizer}, {activation_name}, \"\n","                      f\"tasa de aprendizaje: {learning_rate}, tamaño de capa oculta: {hidden_size}: {train_accuracy:.2f}%\")\n","\n","# Ajustar el espacio entre los subgráficos y mostrar las figuras\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1CWyHeQZR-7vMHQhBHkuc3-p3uzfAa6Vh"},"id":"tFWBeaaz58dR","executionInfo":{"status":"ok","timestamp":1734731959645,"user_tz":0,"elapsed":918879,"user":{"displayName":"Luis Perera Pérez","userId":"06868259129916877352"}},"outputId":"a73cd232-52ec-42ed-dee8-6e4f05124105"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"RBhaw-rCFDaJ"},"execution_count":null,"outputs":[]}]}